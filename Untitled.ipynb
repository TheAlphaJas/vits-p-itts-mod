{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "046825e9-e03a-4fa8-acb1-4f0bb9ee9133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing IndicBERTProcessor on device: cuda\n",
      "Loading IndicBERT model: ai4bharat/indic-bert\n",
      "IndicBERT model and tokenizer loaded successfully.\n",
      "Original Text: नमस्ते दुनिया, यह एक परीक्षण है। एक परीक्षण है एक परीक्षण है\n",
      "Shape of generated embeddings: torch.Size([25, 192])\n",
      "---\n",
      "SUCCESS: Contextual embeddings generated successfully.\n",
      "\n",
      "--- Integration Guide ---\n",
      "1. OBSOLETE FILES: The files `text/symbols.py`, `text/cleaners.py`, and the original `text/__init__.py` are now obsolete and can be removed. The vocabulary and cleaning logic are handled by this new module.\n",
      "\n",
      "2. VITS2 TextEncoder MODIFICATION: Your VITS2 `TextEncoder` module must be modified.\n",
      "   - The original `TextEncoder` likely has an `nn.Embedding` layer as its first component to convert integer IDs to vectors.\n",
      "   - This `nn.Embedding` layer should be REMOVED.\n",
      "   - The `forward` method of your `TextEncoder` should now accept the tensor generated by `text_to_embeddings` directly.\n",
      "   - The output of this new pipeline is already a sequence of vectors, so it can be fed directly into the Transformer blocks of your TextEncoder.\n",
      "\n",
      "3. DATA LOADER: Your dataset's `__getitem__` method should now call `processor.text_to_embeddings(text, lang)` instead of `text_to_sequence(text, cleaner_names)`.\n",
      "   - The collate function will need to handle padding of these embedding tensors if you are batching variable-length sequences.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#\n",
    "# indic_text_handler.py\n",
    "#\n",
    "# This module replaces the traditional Tacotron-style text processing pipeline\n",
    "# (symbol-to-id mapping) with a modern, IndicBERT-based approach. It handles\n",
    "# text normalization, tokenization, and contextual embedding generation for\n",
    "# Indic languages, providing a rich semantic input for the VITS2 model.\n",
    "#\n",
    "# ==============================================================================\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
    "import re\n",
    "\n",
    "# --- Module-level constants ---\n",
    "# Using the canonical multilingual model from AI4Bharat for broad language support.\n",
    "INDIC_BERT_MODEL_NAME = \"ai4bharat/indic-bert\"\n",
    "# VITS2's internal hidden dimension. The BERT embeddings will be projected to this size.\n",
    "# This value should match the `hidden_channels` parameter in your VITS2 config.\n",
    "VITS2_HIDDEN_DIM = 192\n",
    "\n",
    "class IndicBERTProcessor:\n",
    "    \"\"\"\n",
    "    A comprehensive text processor for VITS2 that leverages IndicBERT.\n",
    "\n",
    "    This class encapsulates the entire text-to-embedding pipeline:\n",
    "    1. Normalization: Cleans and canonicalizes Indic script text.\n",
    "    2. Tokenization: Uses the IndicBERT tokenizer.\n",
    "    3. Embedding: Generates contextual word embeddings using the IndicBERT model.\n",
    "    4. Projection: Maps the high-dimensional BERT embeddings to the VITS2 model's\n",
    "       hidden dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        \"\"\"\n",
    "        Initializes the processor by loading the IndicBERT model, tokenizer,\n",
    "        and setting up the text normalizer.\n",
    "\n",
    "        Args:\n",
    "            device (str): The device to run the model on ('cuda' or 'cpu').\n",
    "        \"\"\"\n",
    "        print(f\"Initializing IndicBERTProcessor on device: {device}\")\n",
    "        self.device = device\n",
    "\n",
    "        # 1. Load IndicBERT Tokenizer and Model from Hugging Face\n",
    "        # This model is pre-trained on 12 major Indian languages and is ideal for\n",
    "        # generating rich, context-aware embeddings.\n",
    "        print(f\"Loading IndicBERT model: {INDIC_BERT_MODEL_NAME}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(INDIC_BERT_MODEL_NAME)\n",
    "        self.model = AutoModel.from_pretrained(INDIC_BERT_MODEL_NAME).to(self.device)\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "        print(\"IndicBERT model and tokenizer loaded successfully.\")\n",
    "\n",
    "        # 2. Initialize Indic Text Normalizer\n",
    "        # This is crucial for handling the complexities of Unicode in Indic scripts,\n",
    "        # such as multiple representations for the same character (e.g., nuktas).\n",
    "        self.normalizer_factory = IndicNormalizerFactory()\n",
    "        self.normalizers = {} # Cache for normalizers of different languages\n",
    "\n",
    "        # 3. Define the Projection Layer\n",
    "        # IndicBERT outputs 768-dimensional embeddings. The VITS2 TextEncoder\n",
    "        # typically expects a smaller dimension (e.g., 192 or 256). This linear\n",
    "        # layer bridges that gap.\n",
    "        bert_hidden_dim = self.model.config.hidden_size # Should be 768\n",
    "        self.projection = torch.nn.Linear(bert_hidden_dim, VITS2_HIDDEN_DIM).to(self.device)\n",
    "\n",
    "    def _get_normalizer(self, lang_code):\n",
    "        \"\"\"\n",
    "        Retrieves or creates a normalizer for a given language code.\n",
    "\n",
    "        Args:\n",
    "            lang_code (str): The ISO 639-1 code for the language (e.g., 'hi' for Hindi).\n",
    "\n",
    "        Returns:\n",
    "            An instance of an IndicNormalizer.\n",
    "        \"\"\"\n",
    "        if lang_code not in self.normalizers:\n",
    "            # The `indic-nlp-library` provides script-specific normalization rules.\n",
    "            # `remove_nuktas=False` is generally recommended to preserve phonetic accuracy.\n",
    "            self.normalizers[lang_code] = self.normalizer_factory.get_normalizer(lang_code, remove_nuktas=False)\n",
    "        return self.normalizers[lang_code]\n",
    "\n",
    "    def _clean_text(self, text: str, lang_code: str) -> str:\n",
    "        \"\"\"\n",
    "        Performs normalization and basic cleaning on the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): The raw input text.\n",
    "            lang_code (str): The language code.\n",
    "\n",
    "        Returns:\n",
    "            str: The cleaned and normalized text.\n",
    "        \"\"\"\n",
    "        # Use the script-specific normalizer from indic-nlp-library\n",
    "        normalizer = self._get_normalizer(lang_code)\n",
    "        normalized_text = normalizer.normalize(text)\n",
    "\n",
    "        # Additional basic cleaning: collapse multiple whitespace characters into one\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', normalized_text).strip()\n",
    "        return cleaned_text\n",
    "\n",
    "    def text_to_embeddings(self, text: str, lang_code: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        The main public method to convert a raw text string into a tensor of\n",
    "        contextual embeddings ready for the VITS2 model.\n",
    "\n",
    "        This function replaces the old `text_to_sequence` function.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text string (e.g., \"नमस्ते दुनिया\").\n",
    "            lang_code (str): The language code (e.g., \"hi\").\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of shape\n",
    "                          containing the projected contextual embeddings.\n",
    "        \"\"\"\n",
    "        # Step 1: Clean and normalize the input text\n",
    "        cleaned_text = self._clean_text(text, lang_code)\n",
    "\n",
    "        # Step 2: Tokenize the text using the IndicBERT tokenizer\n",
    "        # We add special tokens (,) as required by BERT-style models.\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            cleaned_text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Step 3: Generate contextual embeddings using IndicBERT\n",
    "        # We use torch.no_grad() to disable gradient calculations, as we are\n",
    "        # only performing inference here, which saves memory and computation.\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**tokenized_inputs)\n",
    "\n",
    "        # The last_hidden_state contains the embeddings for each token.\n",
    "        # Shape: [batch_size, sequence_length, bert_hidden_dim]\n",
    "        last_hidden_state = model_output.last_hidden_state\n",
    "\n",
    "        # Step 4: Project embeddings to the VITS2 hidden dimension\n",
    "        projected_embeddings = self.projection(last_hidden_state)\n",
    "\n",
    "        return projected_embeddings.squeeze(0) # Return shape\n",
    "\n",
    "# ==============================================================================\n",
    "#\n",
    "# Integration Notes & Example Usage\n",
    "#\n",
    "# ==============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Example of how to use the IndicBERTProcessor and explanation of how to\n",
    "    integrate it into your VITS2 training pipeline.\n",
    "    \"\"\"\n",
    "    # --- Example Usage ---\n",
    "    processor = IndicBERTProcessor()\n",
    "    hindi_text = \"नमस्ते दुनिया, यह एक परीक्षण है। एक परीक्षण है एक परीक्षण है\"\n",
    "    language = \"hi\"\n",
    "\n",
    "    # This is the new input for your model's text encoder\n",
    "    embeddings_tensor = processor.text_to_embeddings(hindi_text, language)\n",
    "\n",
    "    print(f\"Original Text: {hindi_text}\")\n",
    "    print(f\"Shape of generated embeddings: {embeddings_tensor.shape}\")\n",
    "    print(\"---\")\n",
    "    print(\"SUCCESS: Contextual embeddings generated successfully.\")\n",
    "    print(\"\\n--- Integration Guide ---\")\n",
    "    print(\"1. OBSOLETE FILES: The files `text/symbols.py`, `text/cleaners.py`, and the original `text/__init__.py` are now obsolete and can be removed. The vocabulary and cleaning logic are handled by this new module.\")\n",
    "    print(\"\\n2. VITS2 TextEncoder MODIFICATION: Your VITS2 `TextEncoder` module must be modified.\")\n",
    "    print(\"   - The original `TextEncoder` likely has an `nn.Embedding` layer as its first component to convert integer IDs to vectors.\")\n",
    "    print(\"   - This `nn.Embedding` layer should be REMOVED.\")\n",
    "    print(\"   - The `forward` method of your `TextEncoder` should now accept the tensor generated by `text_to_embeddings` directly.\")\n",
    "    print(\"   - The output of this new pipeline is already a sequence of vectors, so it can be fed directly into the Transformer blocks of your TextEncoder.\")\n",
    "    print(\"\\n3. DATA LOADER: Your dataset's `__getitem__` method should now call `processor.text_to_embeddings(text, lang)` instead of `text_to_sequence(text, cleaner_names)`.\")\n",
    "    print(\"   - The collate function will need to handle padding of these embedding tensors if you are batching variable-length sequences.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8e16657-de35-4c64-b90f-9a98be853261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m949.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362a6b95-7b1a-4091-b1de-1e692bb19be6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
